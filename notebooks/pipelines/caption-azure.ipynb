{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Import Dependencies\n",
    "\n",
    "import time\n",
    "import hashlib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "import random\n",
    "import pyarrow\n",
    "import requests\n",
    "from PIL import Image\n",
    "import os\n",
    "import datetime as dt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import praw\n",
    "from praw.models import ListingGenerator\n",
    "from tqdm import tqdm\n",
    "from adlfs import AzureBlobFileSystem\n",
    "import os\n",
    "import hashlib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "import random\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Initialize Secrets\n",
    "\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"] = \"ajdevreddit\"\n",
    "\n",
    "os.environ[\n",
    "    \"AZURE_STORAGE_ACCOUNT_KEY\"] = \"+9066TCgdeVignRdy50G4qjmNoUJuibl9ERiTGzdV4fwkvgdV3aSVqgLwldgZxj/UpKLkkfXg+3k+AStjFI33Q==\"\n",
    "\n",
    "os.environ[\n",
    "    \"AZURE_STORE_ACCOUNT_KEY\"] = \"+9066TCgdeVignRdy50G4qjmNoUJuibl9ERiTGzdV4fwkvgdV3aSVqgLwldgZxj/UpKLkkfXg+3k+AStjFI33Q==\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "class AzureFileStorageAdapter(object):\n",
    "    def __init__(self, container_name: str = \"data\"):\n",
    "        self.__account_name: str = os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\n",
    "        self.__account_key: str = os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n",
    "        self.container_name: str = container_name\n",
    "\n",
    "    def get_file_storage(self) -> AzureBlobFileSystem:\n",
    "        return AzureBlobFileSystem(\n",
    "            account_name=self.__account_name,\n",
    "            account_key=self.__account_key,\n",
    "            container_name=self.container_name)\n",
    "\n",
    "    def get_file_storage_root(self) -> AzureBlobFileSystem:\n",
    "        return AzureBlobFileSystem(account_name=self.__account_name, account_key=self.__account_key,\n",
    "                                   container_name=self.container_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "tagging_schema = pyarrow.schema(\n",
    "    [\n",
    "        pyarrow.field(\"id\", pyarrow.string()),\n",
    "        pyarrow.field(\"subreddit\", pyarrow.string()),\n",
    "        pyarrow.field(\"author\", pyarrow.string()),\n",
    "        pyarrow.field(\"title\", pyarrow.string()),\n",
    "        pyarrow.field(\"caption\", pyarrow.string()),\n",
    "        pyarrow.field(\"hash\", pyarrow.string()),\n",
    "        pyarrow.field(\"permalink\", pyarrow.string()),\n",
    "        pyarrow.field(\"original_url\", pyarrow.string()),\n",
    "        pyarrow.field(\"image_name\", pyarrow.string()),\n",
    "        pyarrow.field(\"path\", pyarrow.string()),\n",
    "        pyarrow.field(\"model\", pyarrow.string()),\n",
    "        pyarrow.field(\"exists\", pyarrow.bool_()),\n",
    "        pyarrow.field(\"curated\", pyarrow.bool_()),\n",
    "        pyarrow.field(\"accept\", pyarrow.bool_()),\n",
    "        pyarrow.field(\"tags\", pyarrow.list_(pyarrow.string())),\n",
    "        pyarrow.field(\"azure_caption\", pyarrow.string()),\n",
    "        pyarrow.field(\"thumbnail_path\", pyarrow.string()),\n",
    "        pyarrow.field(\"thumbnail_exists\", pyarrow.bool_()),\n",
    "        pyarrow.field(\"thumbnail_curated\", pyarrow.bool_()),\n",
    "        pyarrow.field(\"thumbnail_accept\", pyarrow.bool_()),\n",
    "        pyarrow.field(\"additional_captions\", pyarrow.list_(pyarrow.string())),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "file_system: AzureBlobFileSystem = AzureFileStorageAdapter('data').get_file_storage()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "thumbnail_curation = pandas.read_parquet(\"data/parquet/thumbnail_curation.parquet\", engine='pyarrow', filesystem=file_system, schema=tagging_schema)\n",
    "\n",
    "display(thumbnail_curation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "all_data_from_captions = pandas.read_parquet(\"data/parquet/all_data_from_captions.parquet\", engine='pyarrow', filesystem=file_system)\n",
    "all_data_from_captions.set_index('id', inplace=True, drop=False)\n",
    "\n",
    "filtered_data_from_captions = pandas.read_parquet(\"data/parquet/filtered_data_from_captions.parquet\", engine='pyarrow', filesystem=file_system)\n",
    "filtered_data_from_captions.set_index('id', inplace=True, drop=False)\n",
    "\n",
    "display(all_data_from_captions)\n",
    "display(filtered_data_from_captions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "import json\n",
    "\n",
    "display(\"=== Obtaining Caption Files List ===\")\n",
    "current_captions = file_system.ls(\"data/caption\")\n",
    "display(f\"current caption files: {len(current_captions)}\")\n",
    "\n",
    "all_data = []\n",
    "filtered_data = []\n",
    "\n",
    "for caption_file in tqdm(current_captions, total=len(current_captions), desc='Reading caption files'):\n",
    "    image_id = caption_file.split('/')[-1].split('.')[0]\n",
    "    if image_id in all_data_from_captions['id'].values or image_id in filtered_data_from_captions['id'].values:\n",
    "        continue\n",
    "    try:\n",
    "        file_size = file_system.size(caption_file)\n",
    "        if file_size == 0:\n",
    "            display(f'Empty file -- removing {image_id}', clear=True)\n",
    "            file_system.rm(caption_file)\n",
    "            continue\n",
    "        caption_data = json.loads(file_system.read_text(caption_file, encoding='utf-8'))\n",
    "        caption_data[\"id\"] = image_id\n",
    "        dense_caption_result = caption_data.get('denseCaptionsResult')\n",
    "        metadata = caption_data.get('metadata')\n",
    "        tags_result = caption_data.get('tagsResult')\n",
    "        smart_crop_result = caption_data.get('smartCropsResult')\n",
    "        basic_caption = caption_data.get('captionResult')\n",
    "        _filtered_data = {\n",
    "            \"id\": image_id,\n",
    "            \"captions\": [basic_caption],\n",
    "            \"dense_captions\": dense_caption_result['values'],\n",
    "            \"meta\": [metadata],\n",
    "            \"tags\": tags_result['values'],\n",
    "            \"smart_crop\": smart_crop_result['values']\n",
    "        }\n",
    "        all_data.append(caption_data)\n",
    "        filtered_data.append(_filtered_data)\n",
    "    except Exception as e:\n",
    "        display(f\" Exception {e} for {image_id}\", clear=True)\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "new_all_data_from_captions = pandas.DataFrame(data=all_data)\n",
    "\n",
    "new_filtered_data_from_captions = pandas.DataFrame(data=filtered_data)\n",
    "\n",
    "display(new_all_data_from_captions)\n",
    "\n",
    "display(new_filtered_data_from_captions)\n",
    "\n",
    "combined_new_all_data_from_captions = pandas.concat([all_data_from_captions, new_all_data_from_captions])\n",
    "\n",
    "combined_new_filtered_data_from_captions = pandas.concat([filtered_data_from_captions, new_filtered_data_from_captions])\n",
    "\n",
    "display(combined_new_all_data_from_captions)\n",
    "\n",
    "display(combined_new_filtered_data_from_captions)\n",
    "\n",
    "combined_new_all_data_from_captions.to_parquet(\"data/parquet/all_data_from_captions.parquet\", engine='pyarrow', filesystem=file_system)\n",
    "\n",
    "combined_new_filtered_data_from_captions.to_parquet(\"data/parquet/filtered_data_from_captions.parquet\", engine='pyarrow', filesystem=file_system)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "all_data_from_captions = pandas.read_parquet(\"data/parquet/all_data_from_captions.parquet\", engine='pyarrow', filesystem=file_system)\n",
    "display(all_data_from_captions)\n",
    "\n",
    "filtered_data_from_captions = pandas.read_parquet(\"data/parquet/filtered_data_from_captions.parquet\", engine='pyarrow', filesystem=file_system)\n",
    "\n",
    "display(filtered_data_from_captions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "\n",
    "tags = pandas.DataFrame({'id': filtered_data_from_captions.id, 'tags': filtered_data_from_captions.tags})\n",
    "out = []\n",
    "for i, r in tqdm(tags.iterrows(), total=len(tags)):\n",
    "    if r['tags'] is None:\n",
    "        continue\n",
    "    tag = pandas.json_normalize(r['tags'])\n",
    "    tag['id'] = r['id']\n",
    "    d = tag.to_dict(orient='records')\n",
    "    out.extend(d)\n",
    "converted_tags = pandas.DataFrame(data=out)\n",
    "display(converted_tags)\n",
    "converted_tags.to_parquet(\"data/parquet/tags.parquet\", engine='pyarrow', filesystem=file_system)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "dense = pandas.DataFrame({'id': filtered_data_from_captions.id, 'dense_captions': filtered_data_from_captions.dense_captions})\n",
    "\n",
    "out = []\n",
    "for i, r in tqdm(dense.iterrows(), total=len(dense)):\n",
    "    if r['dense_captions'] is None:\n",
    "        continue\n",
    "    dense_caption = pandas.json_normalize(r['dense_captions'])\n",
    "    dense_caption['id'] = r['id']\n",
    "    d = dense_caption.to_dict(orient='records')\n",
    "    out.extend(d)\n",
    "\n",
    "converted = pandas.DataFrame(data=out)\n",
    "display(converted)\n",
    "converted.to_parquet(\"data/parquet/dense_captions.parquet\", engine='pyarrow', filesystem=file_system)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "single_caption_data = pandas.DataFrame(\n",
    "    {\n",
    "        'id': all_data_from_captions['id'],\n",
    "        'azure_caption': [item['text'] for item in all_data_from_captions['captionResult']],\n",
    "        'tags': [[foo['name'] for foo in item['values']] for item in all_data_from_captions['tagsResult']]\n",
    "    })\n",
    "\n",
    "single_caption_data_indexed = single_caption_data.set_index(\"id\")\n",
    "\n",
    "for index, row in tqdm(single_caption_data_indexed.iterrows(), total=len(single_caption_data_indexed)):\n",
    "    print(index)\n",
    "\n",
    "display(\"== Updated With Basic Captions ==\")\n",
    "display(thumbnail_curation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "accepted_final.to_parquet(\"data/parquet/curation_2.parquet\", engine='pyarrow', filesystem=file_system, schema=tagging_schema)\n",
    "del accepted_final"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "accepted_final = pandas.read_parquet(\"data/parquet/curation_2.parquet\", engine='pyarrow', filesystem=file_system, schema=tagging_schema)\n",
    "display(accepted_final)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "def get_aspect_ratio(x: object):\n",
    "    return x['crops'][0]['aspectRatio']\n",
    "\n",
    "\n",
    "def get_bounding_box(x: object):\n",
    "    return x['crops'][0]['boundingBox']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "%%time\n",
    "\n",
    "cropping = pandas.DataFrame(\n",
    "    {'id': filtered_data_from_captions['id'], 'crops': filtered_data_from_captions['smart_crop']}).set_index('id',\n",
    "                                                                                                             drop=False)\n",
    "\n",
    "cropping['aspectRatio'] = cropping.progress_apply(lambda x: get_aspect_ratio(x), axis=1)\n",
    "cropping['bounding_box'] = cropping.progress_apply(lambda x: get_bounding_box(x), axis=1)\n",
    "cropping['x'] = cropping.progress_apply(lambda x: x['bounding_box']['x'], axis=1)\n",
    "cropping['y'] = cropping.progress_apply(lambda x: x['bounding_box']['y'], axis=1)\n",
    "cropping['w'] = cropping.progress_apply(lambda x: x['bounding_box']['w'], axis=1)\n",
    "cropping['h'] = cropping.progress_apply(lambda x: x['bounding_box']['h'], axis=1)\n",
    "\n",
    "display(cropping)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "%%time\n",
    "\n",
    "\n",
    "def create_thumbnail(target_image_id: str, file_names: list, crops: pandas.DataFrame, curated_data: pandas.DataFrame):\n",
    "    _file_system: AzureBlobFileSystem = AzureFileStorageAdapter('data').get_file_storage()\n",
    "\n",
    "    out_path = f\"data/image/thumbnail/{target_image_id}.jpg\"\n",
    "    try:\n",
    "        if target_image_id is None or out_path in file_names:\n",
    "            # print(f'Image {target_image_id} already exists, skipping')\n",
    "            return out_path\n",
    "\n",
    "        cropping_information = cropping.loc[crops['id'] == target_image_id]\n",
    "        if cropping_information is None or len(cropping_information) == 0:\n",
    "            # print(f'No cropping information for {target_image_id}, skipping')\n",
    "            return \"/data/nope\"\n",
    "\n",
    "        record = curated_data.loc[curated_data['id'] == target_image_id]\n",
    "        record_path = record.to_dict(orient='records')[0]['path']\n",
    "        image_url = file_system.url(record_path)\n",
    "        original_image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "        copied_image = original_image.copy()\n",
    "        original_image.close()\n",
    "\n",
    "        cropped = copied_image.crop((cropping_information['x'].values[0],\n",
    "                                     cropping_information['y'].values[0],\n",
    "                                     cropping_information['x'].values[0] +\n",
    "                                     cropping_information['w'].values[0],\n",
    "                                     cropping_information['y'].values[0] +\n",
    "                                     cropping_information['h'].values[0]))\n",
    "        copied_image.close()\n",
    "\n",
    "        resized = cropped.resize((512, 512), 1)\n",
    "        resized.save('temp.jpg')\n",
    "        resized.close()\n",
    "        file_system.upload('temp.jpg', out_path, overwrite=True)\n",
    "        print(f'Thumbnail created for {target_image_id}')\n",
    "        return out_path\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f'Error creating thumbnail for {target_image_id}: {ex}')\n",
    "        return \"/data/nope\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "%%time\n",
    "\n",
    "\n",
    "file_names = file_system.ls('data/image/thumbnail')\n",
    "\n",
    "accepted_final['thumbnail_path'] = accepted_final.progress_apply(\n",
    "    lambda x: create_thumbnail(x['id'], file_names, cropping, accepted_final), axis=1)\n",
    "\n",
    "display(accepted_final)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "accepted_final['thumbnail_exists'] = accepted_final.progress_apply(lambda x: file_system.exists(x['thumbnail_path']),\n",
    "                                                                   axis=1)\n",
    "\n",
    "display(accepted_final)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "accepted_copy = accepted_final.copy()\n",
    "\n",
    "accepted_copy['additional_captions'] = accepted_copy.progress_apply(lambda x: [], axis=1)\n",
    "\n",
    "accepted_slice = accepted_copy.loc[accepted_copy['thumbnail_exists'] == True, tagging_schema.names]\n",
    "\n",
    "accepted_slice.dropna(inplace=True)\n",
    "\n",
    "accepted_slice.reset_index(inplace=True, drop=True)\n",
    "\n",
    "display(accepted_slice)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title TODO\n",
    "\n",
    "%%time\n",
    "\n",
    "\n",
    "current_data.set_index('id', inplace=True, drop=False)\n",
    "accepted_slice.set_index('id', inplace=True, drop=False)\n",
    "\n",
    "filtered = accepted_slice.loc(accepted_slice['id'].isnotin(current_data['id']))\n",
    "filtered.dropna(inplace=True)\n",
    "filtered.reset_index(inplace=True, drop=True)\n",
    "display(filtered)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
