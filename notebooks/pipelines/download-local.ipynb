{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-11T18:35:12.336514400Z",
     "start_time": "2023-06-11T18:35:12.213491500Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Install Dependancies\n",
    "\n",
    "# !pip install adlfs > /dev/null 2>&1 && echo $?\n",
    "# !pip install pyarrow > /dev/null 2>&1 && echo $?\n",
    "# !pip install praw > /dev/null 2>&1 && echo $?\n",
    "# !pip install transformers > /dev/null 2>&1 && echo $?\n",
    "# !pip install retry > /dev/null 2 > & 1 & & echo $?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# @title Import Dependancies\n",
    "\n",
    "\n",
    "import time\n",
    "from retry import retry\n",
    "import hashlib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "import random\n",
    "import pyarrow\n",
    "import requests\n",
    "from PIL import Image\n",
    "import os\n",
    "import datetime as dt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import praw\n",
    "from praw.models import ListingGenerator\n",
    "from tqdm import tqdm\n",
    "from adlfs import AzureBlobFileSystem\n",
    "import os\n",
    "import hashlib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "import random\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "from retry import retry\n",
    "from retry.api import retry_call\n",
    "import requests\n",
    "from PIL import Image\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T18:35:12.401178200Z",
     "start_time": "2023-06-11T18:35:12.218511500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# @title Initialize Secrets\n",
    "\n",
    "reddit: praw.Reddit = praw.Reddit(client_id='5hVavL0PIRyM_1JSvqT6UQ', client_secret='BjD2kS3WNLnJc59RKY-JJUuc_Z9-JA',\n",
    "\t\t\t\t\t\t\t\t  user_agent='script:%(bot_name)s:v%(bot_version)s (by /u/%(bot_author)s)',\n",
    "\t\t\t\t\t\t\t\t  check_for_async=False)\n",
    "\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"] = \"ajdevreddit\"\n",
    "\n",
    "os.environ[\n",
    "\t\"AZURE_STORAGE_ACCOUNT_KEY\"] = \"+9066TCgdeVignRdy50G4qjmNoUJuibl9ERiTGzdV4fwkvgdV3aSVqgLwldgZxj/UpKLkkfXg+3k+AStjFI33Q==\"\n",
    "\n",
    "os.environ[\n",
    "\t\"AZURE_STORE_ACCOUNT_KEY\"] = \"+9066TCgdeVignRdy50G4qjmNoUJuibl9ERiTGzdV4fwkvgdV3aSVqgLwldgZxj/UpKLkkfXg+3k+AStjFI33Q==\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T18:35:12.401178200Z",
     "start_time": "2023-06-11T18:35:12.247492400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# @title Initalize Helpers\n",
    "\n",
    "\n",
    "class BlipCaption:\n",
    "\tdef __init__(self, device_name: str = \"cuda\"):\n",
    "\t\tself.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\t\tself.device = torch.device(device_name)\n",
    "\t\tself.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  torch_dtype=torch.float16).to(self.device)\n",
    "\n",
    "\tdef caption_image(self, image_path: str) -> str:\n",
    "\t\ttry:\n",
    "\t\t\traw_image = Image.open(image_path).convert('RGB')\n",
    "\t\t\tinputs = self.processor(raw_image, return_tensors=\"pt\").to(self.device, torch.float16)\n",
    "\t\t\tout = self.model.generate(**inputs)\n",
    "\t\t\treturn self.processor.decode(out[0], skip_special_tokens=True, max_new_tokens=50)\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(e)\n",
    "\t\t\treturn \"\"\n",
    "\n",
    "\n",
    "class Functions(object):\n",
    "\t@retry(Exception, tries=5, delay=1, jitter=1)\n",
    "\tdef make_trouble(self, url):\n",
    "\t\treturn requests.get(url)\n",
    "\n",
    "\tdef get_hash_from_path(self, in_path: str):\n",
    "\t\tif os.path.exists(in_path):\n",
    "\t\t\twith open(in_path, 'rb') as f_:\n",
    "\t\t\t\tcontent = f_.read()\n",
    "\t\t\t\tresult = hashlib.md5(content).hexdigest()\n",
    "\t\t\t\treturn result, content\n",
    "\t\telse:\n",
    "\t\t\treturn \"\"\n",
    "\n",
    "\tdef fetch_image(self, x: object, file_list__, file_system__) -> str:\n",
    "\t\ttry:\n",
    "\t\t\turl = x['original_url']\n",
    "\t\t\tsubreddit = x['subreddit']\n",
    "\t\t\timage_id = x['id']\n",
    "\t\t\tos.makedirs(f\"temp/image/{subreddit}\", exist_ok=True)\n",
    "\t\t\ttemp_path = f\"temp/image/{subreddit}/{image_id}.jpg\"\n",
    "\t\t\tout_path = f\"data/image/{image_id}.jpg\"\n",
    "\t\t\tif file_system__.exists(out_path) and file_system__.size(out_path) > 0:\n",
    "\t\t\t\tprint(f\"exists {image_id}, downloading to {temp_path}\")\n",
    "\t\t\t\tfile_system__.download(out_path, temp_path)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(f\"File is empty for {image_id}\")\n",
    "\t\t\t\tfile_system__.delete(out_path)\n",
    "\t\t\tif os.path.exists(temp_path):\n",
    "\t\t\t\tmd5, content = self.get_hash_from_path(temp_path)\n",
    "\t\t\t\tif md5 != \"f17b01901c752c1bb04928131d1661af\" or md5 != \"d835884373f4d6c8f24742ceabe74946\":\n",
    "\t\t\t\t\tif out_path in file_list__:\n",
    "\t\t\t\t\t\treturn out_path\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tfile_system__.upload(temp_path, out_path, overwrite=True)\n",
    "\t\t\t\t\t\treturn out_path\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treturn \"\"\n",
    "\t\t\telse:\n",
    "\t\t\t\tresponse = self.make_trouble(url)\n",
    "\t\t\t\tmd5 = hashlib.md5(response.content).hexdigest()\n",
    "\t\t\t\tif md5 != \"f17b01901c752c1bb04928131d1661af\" or md5 != \"d835884373f4d6c8f24742ceabe74946\":\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\traw_image = Image.open(BytesIO(response.content))\n",
    "\t\t\t\t\t\traw_image.save(temp_path)\n",
    "\t\t\t\t\t\traw_image.close()\n",
    "\t\t\t\t\t\tif out_path in file_list__:\n",
    "\t\t\t\t\t\t\treturn out_path\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tfile_system__.upload(temp_path, out_path)\n",
    "\t\t\t\t\t\t\treturn out_path\n",
    "\t\t\t\t\texcept Exception as ex:\n",
    "\t\t\t\t\t\tmessage = f\"{x['id']}, {x['subreddit']}, Failure in fetch_image, {ex}\"\n",
    "\t\t\t\t\t\tprint(message)\n",
    "\t\t\t\t\t\treturn \"\"\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treturn \"\"\n",
    "\t\texcept Exception as ex:\n",
    "\t\t\tmessage = f\"{x['id']}, {x['subreddit']}, Failure in fetch_image, {ex}\"\n",
    "\t\t\tprint(message)\n",
    "\t\t\treturn \"\"\n",
    "\n",
    "\tdef get_name_for_image(self, x: object, file_list__) -> str:\n",
    "\t\tpath = x['path']\n",
    "\t\tif path != \"\" and path in file_list__:\n",
    "\t\t\treturn os.path.basename(path)\n",
    "\t\telse:\n",
    "\t\t\treturn \"\"\n",
    "\n",
    "\tdef set_exists(self, x: object, file_system__) -> bool:\n",
    "\t\ttry:\n",
    "\t\t\tsub_reddit = x['subreddit']\n",
    "\t\t\trecord_id = x['id']\n",
    "\t\t\tremote_path = x['path']\n",
    "\t\t\treturn file_system__.exists(remote_path)\n",
    "\t\texcept Exception as ex:\n",
    "\t\t\treturn False\n",
    "\n",
    "\tdef set_hash(self, x: object):\n",
    "\t\tsub_reddit = x['subreddit']\n",
    "\t\trecord_id = x['id']\n",
    "\t\ttemp_path = f\"temp/image/{sub_reddit}/{record_id}.jpg\"\n",
    "\t\tif os.path.exists(temp_path):\n",
    "\t\t\treturn hashlib.md5(open(temp_path, 'rb').read()).hexdigest()\n",
    "\t\telse:\n",
    "\t\t\treturn \"\"\n",
    "\n",
    "\tdef add_source(self, x: object, source_list) -> str:\n",
    "\t\tsub_reddit = x['subreddit']\n",
    "\t\tfor source in source_list:\n",
    "\t\t\tdata_source = source['data']\n",
    "\t\t\tsource_name = source['name']\n",
    "\t\t\tif sub_reddit in data_source:\n",
    "\t\t\t\treturn source_name\n",
    "\t\treturn \"\"\n",
    "\n",
    "\tdef write_log_message(self, submission_id: str, subreddit: str, message: str, exception: Exception) -> str:\n",
    "\t\tdate_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\t\treturn f\"{date_time}\\t{subreddit}\\t{submission_id}\\t{message}\\t{exception}\\n\"\n",
    "\n",
    "\tdef apply_caption(self, x: object, caption_routine: list[BlipCaption], file_system: AzureBlobFileSystem) -> str:\n",
    "\t\texists = x['exists']\n",
    "\t\tif not exists:\n",
    "\t\t\treturn \"\"\n",
    "\t\tsub_reddit = x['subreddit']\n",
    "\t\trecord_id = x['id']\n",
    "\t\tremote_path = x['path']\n",
    "\t\tos.makedirs(f\"temp/image/{sub_reddit}/\", exist_ok=True)\n",
    "\t\ttemp_path = f\"temp/image/{sub_reddit}/{record_id}.jpg\"\n",
    "\n",
    "\t\tif os.path.exists(temp_path):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tresult = random.choice(caption_routine).caption_image(temp_path)\n",
    "\t\t\t\treturn result\n",
    "\t\t\texcept Exception as ex:\n",
    "\t\t\t\tmessage = self.write_log_message(x['id'], x['subreddit'], \"Failure in apply_caption\", ex)\n",
    "\t\t\t\tprint(message)\n",
    "\t\t\t\treturn \"\"\n",
    "\t\telse:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tfile_system.download(remote_path, temp_path)\n",
    "\t\t\t\tresult = random.choice(caption_routine).caption_image(temp_path)\n",
    "\t\t\t\treturn result\n",
    "\t\t\texcept Exception as ex:\n",
    "\t\t\t\tmessage = self.write_log_message(x['id'], x['subreddit'], \"Failure in apply_caption\", ex)\n",
    "\t\t\t\tprint(message)\n",
    "\t\t\t\treturn \"\"\n",
    "\n",
    "\tdef fix_path(self, x: object, fl) -> str:\n",
    "\t\tcurrent_path = x['path']\n",
    "\t\texists = x['exists']\n",
    "\t\tif current_path in fl:\n",
    "\t\t\treturn current_path\n",
    "\t\telse:\n",
    "\t\t\timage_id = x['id']\n",
    "\t\t\tif exists:\n",
    "\t\t\t\treturn f\"data/image/{image_id}.jpg\"\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn \"\"\n",
    "\t\tcurrent_path = x['path']\n",
    "\t\texists = x['exists']\n",
    "\t\tif current_path in fl:\n",
    "\t\t\treturn current_path\n",
    "\t\telse:\n",
    "\t\t\timage_id = x['id']\n",
    "\t\t\tif exists:\n",
    "\t\t\t\treturn f\"data/image/{image_id}.jpg\"\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn \"\"\n",
    "\n",
    "\n",
    "class AzureFileStorageAdapter(object):\n",
    "\tdef __init__(self, container_name: str = \"data\"):\n",
    "\t\tself.__account_name: str = os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\n",
    "\t\tself.__account_key: str = os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n",
    "\t\tself.container_name: str = container_name\n",
    "\n",
    "\tdef get_file_storage(self) -> AzureBlobFileSystem:\n",
    "\t\treturn AzureBlobFileSystem(\n",
    "\t\t\taccount_name=self.__account_name,\n",
    "\t\t\taccount_key=self.__account_key,\n",
    "\t\t\tcontainer_name=self.container_name)\n",
    "\n",
    "\tdef get_file_storage_root(self) -> AzureBlobFileSystem:\n",
    "\t\treturn AzureBlobFileSystem(account_name=self.__account_name, account_key=self.__account_key,\n",
    "\t\t\t\t\t\t\t\t   container_name=self.container_name)\n",
    "\n",
    "\n",
    "schema = pyarrow.schema(\n",
    "\t[\n",
    "\t\tpyarrow.field(\"id\", pyarrow.string()),\n",
    "\t\tpyarrow.field(\"subreddit\", pyarrow.string()),\n",
    "\t\tpyarrow.field(\"author\", pyarrow.string()),\n",
    "\t\tpyarrow.field(\"title\", pyarrow.string()),\n",
    "\t\tpyarrow.field(\"caption\", pyarrow.string()),\n",
    "\t\tpyarrow.field(\"hash\", pyarrow.string()),\n",
    "\t\tpyarrow.field(\"permalink\", pyarrow.string()),\n",
    "\t\tpyarrow.field(\"original_url\", pyarrow.string()),\n",
    "\t\tpyarrow.field(\"image_name\", pyarrow.string()),\n",
    "\t\tpyarrow.field(\"path\", pyarrow.string()),\n",
    "\t\tpyarrow.field(\"model\", pyarrow.string()),\n",
    "\t\tpyarrow.field(\"exists\", pyarrow.bool_()),\n",
    "\t\tpyarrow.field(\"curated\", pyarrow.bool_()),\n",
    "\t\tpyarrow.field(\"accept\", pyarrow.bool_()),\n",
    "\t\tpyarrow.field(\"tags\", pyarrow.list_(pyarrow.string()))\n",
    "\t]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T18:35:12.401178200Z",
     "start_time": "2023-06-11T18:35:12.264490Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'==== Starting Image Acquisition ===='"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Image Acquisition Flow\n",
    "\n",
    "display(\"==== Starting Image Acquisition ====\")\n",
    "\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "file_system = AzureFileStorageAdapter('data').get_file_storage()\n",
    "\n",
    "functions: Functions = Functions()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T18:35:12.402198900Z",
     "start_time": "2023-06-11T18:35:12.295491800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'== Current Subs =='"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'[\"tightdresses\", \"Dresses\", \"SlitDresses\", \"CollaredDresses\", \"DressesPorn\", \"WomenInLongDresses\", \"TrueFMK\", \"DLAH\", \"SFWRedheads\", \"sfwpetite\", \"SFWNextDoorGirls\", \"realasians\", \"KoreanHotties\", \"prettyasiangirls\", \"AsianOfficeLady\", \"AsianInvasion\", \"AesPleasingAsianGirlssexygirls\", \"PrettyGirls\", \"gentlemanboners\", \"hotofficegirls\"]'"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Set Targets\n",
    "\n",
    "\n",
    "# \"memes\",\"CityPorn\", \"EarthPorn\", \"spaceporn\",\t\"itookapicture\", \"trippinthroughtime\", \"bathandbodyworks\",\t\"mildlypenis\",\t\"fatsquirrelhate\",\n",
    "subs = [\n",
    "\t\"tightdresses\",\n",
    "\t\"Dresses\",\n",
    "\t\"SlitDresses\",\n",
    "\t\"CollaredDresses\",\n",
    "\t\"DressesPorn\",\n",
    "\t\"WomenInLongDresses\",\n",
    "\t\"TrueFMK\",\n",
    "\t\"DLAH\",\n",
    "\t\"SFWRedheads\",\n",
    "\t\"sfwpetite\",\n",
    "\t\"SFWNextDoorGirls\",\n",
    "\t\"realasians\",\n",
    "\t\"KoreanHotties\",\n",
    "\t\"prettyasiangirls\",\n",
    "\t\"AsianOfficeLady\",\n",
    "\t\"AsianInvasion\",\n",
    "\t\"AesPleasingAsianGirls\"\n",
    "\t\"sexygirls\",\n",
    "\t\"PrettyGirls\",\n",
    "\t\"gentlemanboners\",\n",
    "\t\"hotofficegirls\",\n",
    "]\n",
    "\n",
    "display(\"== Current Subs ==\")\n",
    "display(json.dumps(subs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T18:35:12.403178Z",
     "start_time": "2023-06-11T18:35:12.310490300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'== Sources To Apply =='"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(20, 2)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                      name                                               data\n0            CityDiffusion                                         [CityPorn]\n1          NatureDiffusion                                        [EarthPorn]\n2          CosmicDiffusion                                        [spaceporn]\n3            ITAPDiffusion                                    [itookapicture]\n4            MemeDiffusion                                            [memes]\n5             TTTDiffusion                               [trippinthroughtime]\n6      WallStreetDiffusion                                   [wallstreetbets]\n7            SexyDiffusion  [selfies, Amicute, amihot, AmIhotAF, HotGirlNe...\n8     FatSquirrelDiffusion                                  [fatsquirrelhate]\n9       CelebrityDiffusion                                      [celebrities]\n10        OldLadyDiffusion                              [oldladiesbakingpies]\n11               SWFPetite                                        [sfwpetite]\n12                SFWMilfs                            [cougars_and_milfs_sfw]\n13        RedHeadDiffusion                                      [SFWRedheads]\n14  NextDoorGirlsDiffusion                                 [SFWNextDoorGirls]\n15      SexyDressDiffusion  [SunDressesGoneWild, ShinyDresses, SlitDresses...\n16      SexyAsianDiffusion  [realasians, KoreanHotties, prettyasiangirls, ...\n17    MildlyPenisDiffusion                                      [mildlypenis]\n18     PrettyGirlDiffusion  [sexygirls, PrettyGirls, gentlemanbonershotoff...\n19         CandleDiffusion                                 [bathandbodyworks]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CityDiffusion</td>\n      <td>[CityPorn]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NatureDiffusion</td>\n      <td>[EarthPorn]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CosmicDiffusion</td>\n      <td>[spaceporn]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ITAPDiffusion</td>\n      <td>[itookapicture]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MemeDiffusion</td>\n      <td>[memes]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>TTTDiffusion</td>\n      <td>[trippinthroughtime]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>WallStreetDiffusion</td>\n      <td>[wallstreetbets]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>SexyDiffusion</td>\n      <td>[selfies, Amicute, amihot, AmIhotAF, HotGirlNe...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>FatSquirrelDiffusion</td>\n      <td>[fatsquirrelhate]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>CelebrityDiffusion</td>\n      <td>[celebrities]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>OldLadyDiffusion</td>\n      <td>[oldladiesbakingpies]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>SWFPetite</td>\n      <td>[sfwpetite]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>SFWMilfs</td>\n      <td>[cougars_and_milfs_sfw]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>RedHeadDiffusion</td>\n      <td>[SFWRedheads]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>NextDoorGirlsDiffusion</td>\n      <td>[SFWNextDoorGirls]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>SexyDressDiffusion</td>\n      <td>[SunDressesGoneWild, ShinyDresses, SlitDresses...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>SexyAsianDiffusion</td>\n      <td>[realasians, KoreanHotties, prettyasiangirls, ...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>MildlyPenisDiffusion</td>\n      <td>[mildlypenis]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>PrettyGirlDiffusion</td>\n      <td>[sexygirls, PrettyGirls, gentlemanbonershotoff...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>CandleDiffusion</td>\n      <td>[bathandbodyworks]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Set Sources\n",
    "\n",
    "\n",
    "sources = [\n",
    "\t{\"name\": \"CityDiffusion\", \"data\": [\"CityPorn\"]},\n",
    "\t{\"name\": \"NatureDiffusion\", \"data\": [\"EarthPorn\"]},\n",
    "\t{\"name\": \"CosmicDiffusion\", \"data\": [\"spaceporn\"]},\n",
    "\t{\"name\": \"ITAPDiffusion\", \"data\": [\"itookapicture\"]},\n",
    "\t{\"name\": \"MemeDiffusion\", \"data\": [\"memes\"]},\n",
    "\t{\"name\": \"TTTDiffusion\", \"data\": [\"trippinthroughtime\"]},\n",
    "\t{\"name\": \"WallStreetDiffusion\", \"data\": [\"wallstreetbets\"]},\n",
    "\t{\"name\": \"SexyDiffusion\", \"data\": [\"selfies\", \"Amicute\", \"amihot\", \"AmIhotAF\", \"HotGirlNextDoor\"]},\n",
    "\t{\"name\": \"FatSquirrelDiffusion\", \"data\": [\"fatsquirrelhate\"]},\n",
    "\t{\"name\": \"CelebrityDiffusion\", \"data\": [\"celebrities\"]},\n",
    "\t{\"name\": \"OldLadyDiffusion\", \"data\": [\"oldladiesbakingpies\"]},\n",
    "\t{\"name\": \"SWFPetite\", \"data\": [\"sfwpetite\"]},\n",
    "\t{\"name\": \"SFWMilfs\", \"data\": [\"cougars_and_milfs_sfw\"]},\n",
    "\t{\"name\": \"RedHeadDiffusion\", \"data\": [\"SFWRedheads\"]},\n",
    "\t{\"name\": \"NextDoorGirlsDiffusion\", \"data\": [\"SFWNextDoorGirls\"]},\n",
    "\t{\"name\": \"SexyDressDiffusion\", \"data\": [\"SunDressesGoneWild\", \"ShinyDresses\", \"SlitDresses\", \"CollaredDresses\", \"DressesPorn\",\"WomenInLongDresses\", \"Dresses\"]},\n",
    "\t{\"name\": \"SexyAsianDiffusion\", \"data\": [\"realasians\", \"KoreanHotties\", \"prettyasiangirls\", \"AsianOfficeLady\", \"AsianInvasion\", \"AesPleasingAsianGirls\"]},\n",
    "\t{\"name\": \"MildlyPenisDiffusion\", \"data\": [\"mildlypenis\"]},\n",
    "\t{\"name\": \"PrettyGirlDiffusion\", \"data\": [\"sexygirls\", \"PrettyGirls\", \"gentlemanboners\" \"hotofficegirls\", \"tightdresses\", \"DLAH\", \"TrueFMK\"]},\n",
    "\t{\"name\": \"CandleDiffusion\", \"data\": [\"bathandbodyworks\"]}\n",
    "]\n",
    "sources_df = pd.DataFrame.from_records(sources)\n",
    "\n",
    "display(\"== Sources To Apply ==\")\n",
    "display(sources_df.shape)\n",
    "display(sources_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T18:35:12.403178Z",
     "start_time": "2023-06-11T18:35:12.329491300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "'== primary_caption =='"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(45600, 15)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "              id subreddit           author   \nid                                            \n100rn7k  100rn7k  AmIhotAF     veritynicole  \\\n1013bdt  1013bdt  AmIhotAF      RaulDea9286   \n105mekt  105mekt  AmIhotAF         lindaniz   \n105qvgl  105qvgl  AmIhotAF      CaitVLove11   \n105rpcj  105rpcj  AmIhotAF  Flashy-Desk1858   \n\n                                                  title   \nid                                                        \n100rn7k             hey, hows your new year going (23F)  \\\n1013bdt                                   36F - ITALIAN   \n105mekt  interesting in good forward relationship (f24)   \n105qvgl                       Laughing is my favorite ðŸ˜†   \n105rpcj        [f22] What do you think when you see me?   \n\n                                                   caption   \nid                                                           \n100rn7k  a woman in a white shirt and black pants is po...  \\\n1013bdt            arafed image of a woman in a bikini top   \n105mekt  a close up of a woman with red hair and a whit...   \n105qvgl  a woman in a blue tank top and shorts is smili...   \n105rpcj    a woman in a blue bikini top and a blue bra top   \n\n                                     hash   \nid                                          \n100rn7k  4bd00c19fa0ff2ade855e6d364b0760b  \\\n1013bdt  7c0d158cba8654ef1c635cbc5471d597   \n105mekt  ba4a0962cca2266a741e1e1700589c04   \n105qvgl  27bfe82c37314a0bcf02ab72eaf3a9e5   \n105rpcj  329eb42b8267fa1cc2980da8e48bcef1   \n\n                                                 permalink   \nid                                                           \n100rn7k  /r/AmIhotAF/comments/100rn7k/hey_hows_your_new...  \\\n1013bdt          /r/AmIhotAF/comments/1013bdt/36f_italian/   \n105mekt  /r/AmIhotAF/comments/105mekt/interesting_in_go...   \n105qvgl  /r/AmIhotAF/comments/105qvgl/laughing_is_my_fa...   \n105rpcj  /r/AmIhotAF/comments/105rpcj/f22_what_do_you_t...   \n\n                                original_url   image_name   \nid                                                          \n100rn7k  https://i.redd.it/n7r47s0gkh9a1.jpg  100rn7k.jpg  \\\n1013bdt  https://i.redd.it/bg0wwdlt5k9a1.jpg  1013bdt.jpg   \n105mekt  https://i.redd.it/4avjshsz8naa1.jpg  105mekt.jpg   \n105qvgl  https://i.redd.it/2pulzr0lxmaa1.jpg  105qvgl.jpg   \n105rpcj  https://i.redd.it/rz68pf934naa1.jpg  105rpcj.jpg   \n\n                           path          model  exists  curated  accept tags  \nid                                                                            \n100rn7k  data/image/100rn7k.jpg  SexyDiffusion    True     True   False   []  \n1013bdt  data/image/1013bdt.jpg  SexyDiffusion    True     True    True   []  \n105mekt  data/image/105mekt.jpg  SexyDiffusion    True     True    True   []  \n105qvgl  data/image/105qvgl.jpg  SexyDiffusion    True     True    True   []  \n105rpcj  data/image/105rpcj.jpg  SexyDiffusion    True     True    True   []  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>subreddit</th>\n      <th>author</th>\n      <th>title</th>\n      <th>caption</th>\n      <th>hash</th>\n      <th>permalink</th>\n      <th>original_url</th>\n      <th>image_name</th>\n      <th>path</th>\n      <th>model</th>\n      <th>exists</th>\n      <th>curated</th>\n      <th>accept</th>\n      <th>tags</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100rn7k</th>\n      <td>100rn7k</td>\n      <td>AmIhotAF</td>\n      <td>veritynicole</td>\n      <td>hey, hows your new year going (23F)</td>\n      <td>a woman in a white shirt and black pants is po...</td>\n      <td>4bd00c19fa0ff2ade855e6d364b0760b</td>\n      <td>/r/AmIhotAF/comments/100rn7k/hey_hows_your_new...</td>\n      <td>https://i.redd.it/n7r47s0gkh9a1.jpg</td>\n      <td>100rn7k.jpg</td>\n      <td>data/image/100rn7k.jpg</td>\n      <td>SexyDiffusion</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1013bdt</th>\n      <td>1013bdt</td>\n      <td>AmIhotAF</td>\n      <td>RaulDea9286</td>\n      <td>36F - ITALIAN</td>\n      <td>arafed image of a woman in a bikini top</td>\n      <td>7c0d158cba8654ef1c635cbc5471d597</td>\n      <td>/r/AmIhotAF/comments/1013bdt/36f_italian/</td>\n      <td>https://i.redd.it/bg0wwdlt5k9a1.jpg</td>\n      <td>1013bdt.jpg</td>\n      <td>data/image/1013bdt.jpg</td>\n      <td>SexyDiffusion</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>105mekt</th>\n      <td>105mekt</td>\n      <td>AmIhotAF</td>\n      <td>lindaniz</td>\n      <td>interesting in good forward relationship (f24)</td>\n      <td>a close up of a woman with red hair and a whit...</td>\n      <td>ba4a0962cca2266a741e1e1700589c04</td>\n      <td>/r/AmIhotAF/comments/105mekt/interesting_in_go...</td>\n      <td>https://i.redd.it/4avjshsz8naa1.jpg</td>\n      <td>105mekt.jpg</td>\n      <td>data/image/105mekt.jpg</td>\n      <td>SexyDiffusion</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>105qvgl</th>\n      <td>105qvgl</td>\n      <td>AmIhotAF</td>\n      <td>CaitVLove11</td>\n      <td>Laughing is my favorite ðŸ˜†</td>\n      <td>a woman in a blue tank top and shorts is smili...</td>\n      <td>27bfe82c37314a0bcf02ab72eaf3a9e5</td>\n      <td>/r/AmIhotAF/comments/105qvgl/laughing_is_my_fa...</td>\n      <td>https://i.redd.it/2pulzr0lxmaa1.jpg</td>\n      <td>105qvgl.jpg</td>\n      <td>data/image/105qvgl.jpg</td>\n      <td>SexyDiffusion</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>105rpcj</th>\n      <td>105rpcj</td>\n      <td>AmIhotAF</td>\n      <td>Flashy-Desk1858</td>\n      <td>[f22] What do you think when you see me?</td>\n      <td>a woman in a blue bikini top and a blue bra top</td>\n      <td>329eb42b8267fa1cc2980da8e48bcef1</td>\n      <td>/r/AmIhotAF/comments/105rpcj/f22_what_do_you_t...</td>\n      <td>https://i.redd.it/rz68pf934naa1.jpg</td>\n      <td>105rpcj.jpg</td>\n      <td>data/image/105rpcj.jpg</td>\n      <td>SexyDiffusion</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Read primary_caption\n",
    "\n",
    "\n",
    "extant_data = pandas.read_parquet(\"data/parquet/primary_caption.parquet\", engine='pyarrow', filesystem=file_system,\n",
    "\t\t\t\t\t\t\t\t  schema=schema)\n",
    "extant_data.set_index('id', inplace=True, drop=False)\n",
    "\n",
    "display(\"== primary_caption ==\")\n",
    "display(extant_data.shape)\n",
    "display(extant_data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T18:35:13.850689700Z",
     "start_time": "2023-06-11T18:35:12.356491200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Temp Dir For Subs...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 3333.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# @title Create Temp Dirs\n",
    "\n",
    "\n",
    "for sub in tqdm(subs, total=len(subs), desc=\"Creating Temp Dir For Subs...\"):\n",
    "\ttemp_dir_path_ = f\"temp/{sub}\"\n",
    "\tif not os.path.exists(temp_dir_path_):\n",
    "\t\tos.makedirs(temp_dir_path_)\n",
    "\tos.makedirs(temp_dir_path_, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T18:35:13.915687Z",
     "start_time": "2023-06-11T18:35:13.849688700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Temp Dir For Subs...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  9.87it/s]\n",
      "Posts - tightdresses - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [00:01<00:00, 97.24it/s]\n",
      "Posts - tightdresses - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [00:00<00:00, 330.07it/s]\n",
      "Posts - tightdresses - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:00<00:00, 309.74it/s]\n",
      "Posts - tightdresses - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [00:00<00:00, 359.74it/s]\n",
      "Posts - tightdresses - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199/199 [00:00<00:00, 365.77it/s]\n",
      "Posts - Dresses - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 1095.69it/s]\n",
      "Posts - Dresses - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:00<00:00, 1169.24it/s]\n",
      "Posts - Dresses - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:00<00:00, 1220.80it/s]\n",
      "Posts - Dresses - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191/191 [00:00<00:00, 1115.86it/s]\n",
      "Posts - Dresses - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 179/179 [00:00<00:00, 1168.60it/s]\n",
      "Posts - SlitDresses - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [00:00<00:00, 1274.89it/s]\n",
      "Posts - SlitDresses - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:00<00:00, 1282.12it/s]\n",
      "Posts - SlitDresses - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [00:00<00:00, 1274.62it/s]\n",
      "Posts - SlitDresses - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 1285.46it/s]\n",
      "Posts - SlitDresses - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 1276.55it/s]\n",
      "Posts - CollaredDresses - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 1090.82it/s]\n",
      "Posts - CollaredDresses - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 1285.12it/s]\n",
      "Posts - CollaredDresses - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 1285.26it/s]\n",
      "Posts - CollaredDresses - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 1342.65it/s]\n",
      "Posts - CollaredDresses - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 1058.96it/s]\n",
      "Posts - DressesPorn - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 1191.85it/s]\n",
      "Posts - DressesPorn - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 1292.04it/s]\n",
      "Posts - DressesPorn - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 1429.86it/s]\n",
      "Posts - DressesPorn - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 1499.09it/s]\n",
      "Posts - DressesPorn - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 1183.92it/s]\n",
      "Posts - WomenInLongDresses - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 1358.36it/s]\n",
      "Posts - WomenInLongDresses - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 765.24it/s]\n",
      "Posts - WomenInLongDresses - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 618.42it/s]\n",
      "Posts - WomenInLongDresses - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 1296.69it/s]\n",
      "Posts - WomenInLongDresses - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 1300.09it/s]\n",
      "Posts - TrueFMK - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [00:01<00:00, 56.11it/s]\n",
      "Posts - TrueFMK - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [00:00<00:00, 230.81it/s]\n",
      "Posts - TrueFMK - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 178/178 [00:00<00:00, 205.80it/s]\n",
      "Posts - TrueFMK - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [00:00<00:00, 244.35it/s]\n",
      "Posts - TrueFMK - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 205.34it/s]\n",
      "Posts - DLAH - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:00<00:00, 238.30it/s]\n",
      "Posts - DLAH - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 678.25it/s]\n",
      "Posts - DLAH - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165/165 [00:00<00:00, 569.89it/s]\n",
      "Posts - DLAH - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 628.67it/s]\n",
      "Posts - DLAH - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [00:00<00:00, 636.04it/s]\n",
      "Posts - SFWRedheads - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [00:04<00:00, 22.87it/s]\n",
      "Posts - SFWRedheads - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [00:03<00:00, 57.48it/s]\n",
      "Posts - SFWRedheads - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 177/177 [00:04<00:00, 42.50it/s]\n",
      "Posts - SFWRedheads - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210/210 [00:03<00:00, 56.03it/s]\n",
      "Posts - SFWRedheads - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [00:03<00:00, 55.40it/s]\n",
      "Posts - sfwpetite - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00, 21.63it/s]\n",
      "Posts - sfwpetite - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:06<00:00, 30.83it/s]\n",
      "Posts - sfwpetite - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 26.84it/s]\n",
      "Posts - sfwpetite - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:05<00:00, 38.64it/s]\n",
      "Posts - sfwpetite - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:05<00:00, 26.12it/s]\n",
      "Posts - SFWNextDoorGirls - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [00:08<00:00, 13.75it/s]\n",
      "Posts - SFWNextDoorGirls - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211/211 [00:05<00:00, 39.47it/s]\n",
      "Posts - SFWNextDoorGirls - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 192/192 [00:06<00:00, 29.45it/s]\n",
      "Posts - SFWNextDoorGirls - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211/211 [00:05<00:00, 38.94it/s]\n",
      "Posts - SFWNextDoorGirls - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 208/208 [00:08<00:00, 24.54it/s]\n",
      "Posts - realasians - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [00:05<00:00, 18.13it/s]\n",
      "Posts - realasians - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [00:07<00:00, 26.53it/s]\n",
      "Posts - realasians - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [00:05<00:00, 29.53it/s]\n",
      "Posts - realasians - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [00:07<00:00, 26.05it/s]\n",
      "Posts - realasians - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:06<00:00, 30.95it/s]\n",
      "Posts - KoreanHotties - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:09<00:00, 10.89it/s]\n",
      "Posts - KoreanHotties - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:16<00:00, 12.23it/s]\n",
      "Posts - KoreanHotties - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [00:11<00:00, 12.84it/s]\n",
      "Posts - KoreanHotties - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:14<00:00, 13.58it/s]\n",
      "Posts - KoreanHotties - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:15<00:00, 12.65it/s]\n",
      "Posts - prettyasiangirls - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107/107 [00:06<00:00, 16.71it/s]\n",
      "Posts - prettyasiangirls - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 207/207 [00:09<00:00, 21.17it/s]\n",
      "Posts - prettyasiangirls - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:09<00:00, 18.55it/s]\n",
      "Posts - prettyasiangirls - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 207/207 [00:10<00:00, 19.80it/s]\n",
      "Posts - prettyasiangirls - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:12<00:00, 15.83it/s]\n",
      "Posts - AsianOfficeLady - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1242.53it/s]\n",
      "Posts - AsianOfficeLady - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 179/179 [00:00<00:00, 422.31it/s]\n",
      "Posts - AsianOfficeLady - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1336.29it/s]\n",
      "Posts - AsianOfficeLady - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191/191 [00:00<00:00, 641.88it/s]\n",
      "Posts - AsianOfficeLady - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1369.85it/s]\n",
      "Posts - AsianInvasion - day: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:00<00:00, 109.57it/s]\n",
      "Posts - AsianInvasion - year: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:02<00:00, 78.41it/s]\n",
      "Posts - AsianInvasion - week: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:00<00:00, 161.48it/s]\n",
      "Posts - AsianInvasion - all: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:02<00:00, 76.46it/s]\n",
      "Posts - AsianInvasion - month: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:00<00:00, 167.13it/s]\n"
     ]
    },
    {
     "ename": "NotFound",
     "evalue": "received 404 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotFound\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 18\u001B[0m\n\u001B[0;32m     15\u001B[0m \tcache_data\u001B[38;5;241m.\u001B[39mset_index(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m, drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sub \u001B[38;5;129;01min\u001B[39;00m subs:\n\u001B[1;32m---> 18\u001B[0m \tsubreddit_stream_2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mreddit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubreddit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdisplay_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m \tsubreddit_stream_3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(reddit\u001B[38;5;241m.\u001B[39msubreddit(display_name\u001B[38;5;241m=\u001B[39msub)\u001B[38;5;241m.\u001B[39mnew(limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m))\n\u001B[0;32m     20\u001B[0m \ttemp_dir_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemp/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msub\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mD:\\code\\repos\\simple-collection\\venv\\lib\\site-packages\\praw\\models\\listing\\generator.py:63\u001B[0m, in \u001B[0;36mListingGenerator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m()\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_list_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing):\n\u001B[1;32m---> 63\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_list_index \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39myielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mD:\\code\\repos\\simple-collection\\venv\\lib\\site-packages\\praw\\models\\listing\\generator.py:89\u001B[0m, in \u001B[0;36mListingGenerator._next_batch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exhausted:\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m()\n\u001B[1;32m---> 89\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reddit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extract_sublist(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing)\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_list_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32mD:\\code\\repos\\simple-collection\\venv\\lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001B[0m, in \u001B[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     36\u001B[0m     arg_string \u001B[38;5;241m=\u001B[39m _generate_arg_string(_old_args[: \u001B[38;5;28mlen\u001B[39m(args)])\n\u001B[0;32m     37\u001B[0m     warn(\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPositional arguments for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m will no longer be\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     39\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m supported in PRAW 8.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCall this function with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marg_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     40\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[0;32m     41\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m     42\u001B[0m     )\n\u001B[1;32m---> 43\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(_old_args, args)), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\code\\repos\\simple-collection\\venv\\lib\\site-packages\\praw\\reddit.py:712\u001B[0m, in \u001B[0;36mReddit.get\u001B[1;34m(self, path, params)\u001B[0m\n\u001B[0;32m    699\u001B[0m \u001B[38;5;129m@_deprecate_args\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    700\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\n\u001B[0;32m    701\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    704\u001B[0m     params: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Dict[\u001B[38;5;28mstr\u001B[39m, Union[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mint\u001B[39m]]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    705\u001B[0m ):\n\u001B[0;32m    706\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001B[39;00m\n\u001B[0;32m    707\u001B[0m \n\u001B[0;32m    708\u001B[0m \u001B[38;5;124;03m    :param path: The path to fetch.\u001B[39;00m\n\u001B[0;32m    709\u001B[0m \u001B[38;5;124;03m    :param params: The query parameters to add to the request (default: ``None``).\u001B[39;00m\n\u001B[0;32m    710\u001B[0m \n\u001B[0;32m    711\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 712\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_objectify_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\code\\repos\\simple-collection\\venv\\lib\\site-packages\\praw\\reddit.py:517\u001B[0m, in \u001B[0;36mReddit._objectify_request\u001B[1;34m(self, data, files, json, method, params, path)\u001B[0m\n\u001B[0;32m    491\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_objectify_request\u001B[39m(\n\u001B[0;32m    492\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    499\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    500\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    501\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Run a request through the ``Objector``.\u001B[39;00m\n\u001B[0;32m    502\u001B[0m \n\u001B[0;32m    503\u001B[0m \u001B[38;5;124;03m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    514\u001B[0m \n\u001B[0;32m    515\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_objector\u001B[38;5;241m.\u001B[39mobjectify(\n\u001B[1;32m--> 517\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    518\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    519\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    520\u001B[0m \u001B[43m            \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    521\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m            \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    525\u001B[0m     )\n",
      "File \u001B[1;32mD:\\code\\repos\\simple-collection\\venv\\lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001B[0m, in \u001B[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     36\u001B[0m     arg_string \u001B[38;5;241m=\u001B[39m _generate_arg_string(_old_args[: \u001B[38;5;28mlen\u001B[39m(args)])\n\u001B[0;32m     37\u001B[0m     warn(\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPositional arguments for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m will no longer be\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     39\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m supported in PRAW 8.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCall this function with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marg_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     40\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[0;32m     41\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m     42\u001B[0m     )\n\u001B[1;32m---> 43\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(_old_args, args)), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\code\\repos\\simple-collection\\venv\\lib\\site-packages\\praw\\reddit.py:941\u001B[0m, in \u001B[0;36mReddit.request\u001B[1;34m(self, data, files, json, method, params, path)\u001B[0m\n\u001B[0;32m    939\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ClientException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt most one of \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is supported.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    940\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 941\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_core\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    942\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    944\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    947\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    948\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    949\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m BadRequest \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[0;32m    950\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\code\\repos\\simple-collection\\venv\\lib\\site-packages\\prawcore\\sessions.py:330\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, path, data, files, json, params, timeout)\u001B[0m\n\u001B[0;32m    328\u001B[0m     json[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapi_type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    329\u001B[0m url \u001B[38;5;241m=\u001B[39m urljoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_requestor\u001B[38;5;241m.\u001B[39moauth_url, path)\n\u001B[1;32m--> 330\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request_with_retries\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    331\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    332\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    335\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    336\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    337\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    338\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\code\\repos\\simple-collection\\venv\\lib\\site-packages\\prawcore\\sessions.py:266\u001B[0m, in \u001B[0;36mSession._request_with_retries\u001B[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001B[0m\n\u001B[0;32m    253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_retry(\n\u001B[0;32m    254\u001B[0m         data,\n\u001B[0;32m    255\u001B[0m         files,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    263\u001B[0m         url,\n\u001B[0;32m    264\u001B[0m     )\n\u001B[0;32m    265\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mSTATUS_EXCEPTIONS:\n\u001B[1;32m--> 266\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mSTATUS_EXCEPTIONS[response\u001B[38;5;241m.\u001B[39mstatus_code](response)\n\u001B[0;32m    267\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m codes[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno_content\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[1;31mNotFound\u001B[0m: received 404 HTTP response"
     ]
    }
   ],
   "source": [
    "# @title Locate And Cache Reddit Data\n",
    "\n",
    "\n",
    "data = []\n",
    "for sub in tqdm(subs, desc=\"Reading Temp Dir For Subs...\"):\n",
    "\tdf = pandas.read_parquet(f\"temp/{sub}\", schema=schema, engine='pyarrow')\n",
    "\trecords = df.to_dict(orient='records')\n",
    "\tdata.extend(records)\n",
    "\n",
    "try:\n",
    "\tcache_data = pandas.DataFrame(data)\n",
    "\tcache_data.set_index('id', drop=False, inplace=True)\n",
    "except KeyError:\n",
    "\tcache_data = pandas.DataFrame(data=[{'id': \"foo\"}])\n",
    "\tcache_data.set_index('id', drop=False, inplace=True)\n",
    "\n",
    "for sub in subs:\n",
    "\tsubreddit_stream_2 = list(reddit.subreddit(display_name=sub).hot(limit=100))\n",
    "\tsubreddit_stream_3 = list(reddit.subreddit(display_name=sub).new(limit=100))\n",
    "\ttemp_dir_path = f\"temp/{sub}\"\n",
    "\tremote_temp_data = f\"data/temp/{sub}\"\n",
    "\tfor time_filter in ['day', 'year', 'week', 'all', 'month']:\n",
    "\t\ttry:\n",
    "\t\t\tsubreddit_stream = reddit.subreddit(display_name=sub).top(limit=100, time_filter=time_filter)\n",
    "\t\t\tsubreddit_stream = list(subreddit_stream)\n",
    "\n",
    "\t\t\tall_streams = subreddit_stream + subreddit_stream_2 + subreddit_stream_3\n",
    "\t\t\tall_streams = list(set(all_streams))\n",
    "\t\t\tp = tqdm(all_streams, total=len(all_streams), desc=f\"Posts - {sub} - {time_filter}\")\n",
    "\t\t\tfor submission in p:\n",
    "\t\t\t\tif submission is None:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tif submission.id in extant_data.index.values or submission.id in cache_data.index.values:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\tif file_system.exists(f\"{remote_temp_data}/{submission.id}.parquet\"):\n",
    "\t\t\t\t\t\t\t\tfile_system.download(f\"{remote_temp_data}/{submission.id}.parquet\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t f\"{temp_dir_path}/{submission.id}.parquet\")\n",
    "\t\t\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\t\t\tsub_id = submission.id\n",
    "\t\t\t\t\t\t\tauthor_name = 'Unknown'\n",
    "\t\t\t\t\t\t\tsubreddit_name = sub\n",
    "\t\t\t\t\t\t\tsub_title = submission.title\n",
    "\t\t\t\t\t\t\tperma_link = submission.permalink\n",
    "\t\t\t\t\t\t\tsub_url = submission.url\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tauthor_name = submission.author.name\n",
    "\t\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\t\tauthor_name = 'Unknown'\n",
    "\t\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t\tp = {\n",
    "\t\t\t\t\t\t\t\t'id': sub_id,\n",
    "\t\t\t\t\t\t\t\t'subreddit': subreddit_name,\n",
    "\t\t\t\t\t\t\t\t'author': author_name,\n",
    "\t\t\t\t\t\t\t\t'title': sub_title,\n",
    "\t\t\t\t\t\t\t\t'caption': '',\n",
    "\t\t\t\t\t\t\t\t'hash': '',\n",
    "\t\t\t\t\t\t\t\t'permalink': perma_link,\n",
    "\t\t\t\t\t\t\t\t'original_url': sub_url,\n",
    "\t\t\t\t\t\t\t\t'image_name': '',\n",
    "\t\t\t\t\t\t\t\t'path': '',\n",
    "\t\t\t\t\t\t\t\t'thumbnail_path': '',\n",
    "\t\t\t\t\t\t\t\t'exists': False,\n",
    "\t\t\t\t\t\t\t\t'curated': False,\n",
    "\t\t\t\t\t\t\t\t'Tags': ['']\n",
    "\t\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\t\td = pd.DataFrame([p])\n",
    "\t\t\t\t\t\t\td.to_parquet(f\"{temp_dir_path}/{submission.id}.parquet\")\n",
    "\t\t\t\t\t\t\td.to_parquet(f\"{remote_temp_data}/{submission.id}.parquet\", filesystem=file_system)\n",
    "\t\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\tlog = f\"{submission.id}, {sub}, Error Writing Post, {e}\"\n",
    "\t\t\t\t\t\t\tprint(log)\n",
    "\t\t\t\t\t\t\tcontinue\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tlog = f\"{sub}, Error Getting Posts For SubReddit, {e}\"\n",
    "\t\t\tprint(log)\n",
    "\t\t\tcontinue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T18:49:08.943682900Z",
     "start_time": "2023-06-11T18:35:13.880687800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Read Cache And Stage Images\n",
    "\n",
    "data = []\n",
    "for sub in tqdm(subs, desc=\"Reading Temp Dir For Subs...\"):\n",
    "\tdf = pandas.read_parquet(f\"temp/{sub}\", schema=schema, engine='pyarrow')\n",
    "\trecords = df.to_dict(orient='records')\n",
    "\tdata.extend(records)\n",
    "\n",
    "temp_data = pandas.DataFrame(data)\n",
    "\n",
    "for i, r in temp_data.iterrows():\n",
    "\ttemp_data.loc[i, 'image_name'] = r.id + \".jpg\"\n",
    "\ttemp_data.loc[i, 'path'] = \"\"\n",
    "\ttemp_data.loc[i, 'hash'] = \"\"\n",
    "\ttemp_data.loc[i, 'caption'] = \"\"\n",
    "\ttemp_data.loc[i, 'model'] = \"\"\n",
    "\ttemp_data.loc[i, 'exists'] = False\n",
    "\ttemp_data.loc[i, 'curated'] = False\n",
    "\ttemp_data.loc[i, 'accept'] = False\n",
    "\ttemp_data.loc[i, 'tags'] = ['']\n",
    "\n",
    "temp_data.set_index('id', inplace=True, drop=False)\n",
    "\n",
    "display(\"==== TEMP DATA ====\")\n",
    "display(temp_data.shape)\n",
    "display(temp_data.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Apply Filter For Images and Existance On Data\n",
    "\n",
    "filtered = temp_data[~temp_data['id'].isin(extant_data.index.values) & (temp_data['original_url'].str.endswith('.jpg') | temp_data['original_url'].str.endswith('.png'))]\n",
    "\n",
    "filtered.dropna(axis=1, how='all')\n",
    "\n",
    "filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "filtered.set_index('id', inplace=True, drop=False)\n",
    "\n",
    "display(\"==== FILTERED DATA ====\")\n",
    "\n",
    "display(filtered.shape)\n",
    "\n",
    "display(filtered.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Add Source To Model\n",
    "\n",
    "with tqdm(total=len(filtered)) as pbar:\n",
    "\tfor i, row in filtered.iterrows():\n",
    "\t\td = functions.add_source(row, sources)\n",
    "\t\tfiltered.loc[i, 'model'] = d\n",
    "\t\tpbar.update()\n",
    "\n",
    "display(\"== Filtered Data With Model ==\")\n",
    "display(filtered.shape)\n",
    "display(filtered.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@retry(Exception, tries=10, delay=3, jitter=2)\n",
    "def make_trouble(url):\n",
    "\treturn requests.get(url)\n",
    "\n",
    "def get_hash_from_path(in_path: str):\n",
    "\tif os.path.exists(in_path):\n",
    "\t\twith open(in_path, 'rb') as f_:\n",
    "\t\t\tcontent = f_.read()\n",
    "\t\t\tresult = hashlib.md5(content).hexdigest()\n",
    "\t\t\treturn result, content\n",
    "\telse:\n",
    "\t\treturn \"\"\n",
    "\n",
    "\n",
    "def fetch_image(x: object, file_list__, file_system__) -> str:\n",
    "\ttry:\n",
    "\t\turl = x['original_url']\n",
    "\t\tsubreddit = x['subreddit']\n",
    "\t\timage_id = x['id']\n",
    "\t\tos.makedirs(f\"temp/image/{subreddit}\", exist_ok=True)\n",
    "\t\ttemp_path = f\"temp/image/{subreddit}/{image_id}.jpg\"\n",
    "\t\tout_path = f\"data/image/{image_id}.jpg\"\n",
    "\t\tif file_system__.exists(out_path):\n",
    "\t\t\tfile_system__.download(out_path, temp_path)\n",
    "\t\tif os.path.exists(temp_path):\n",
    "\t\t\tmd5, content = get_hash_from_path(temp_path)\n",
    "\t\t\tif md5 != \"f17b01901c752c1bb04928131d1661af\" or md5 != \"d835884373f4d6c8f24742ceabe74946\":\n",
    "\t\t\t\tif out_path in file_list__:\n",
    "\t\t\t\t\tfile_system.download(out_path, temp_path)\n",
    "\t\t\t\t\treturn out_path\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treturn out_path\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"Invalid\")\n",
    "\t\t\t\treturn \"\"\n",
    "\t\telse:\n",
    "\t\t\tresponse = requests.get(url)\n",
    "\t\t\tmd5 = hashlib.md5(response.content).hexdigest()\n",
    "\t\t\tif md5 != \"f17b01901c752c1bb04928131d1661af\" or md5 != \"d835884373f4d6c8f24742ceabe74946\":\n",
    "\t\t\t\ttry:\n",
    "\n",
    "\t\t\t\t\traw_image = Image.open(requests.get(url, stream=True).raw)\n",
    "\t\t\t\t\tif raw_image.mode in (\"RGBA\", \"P\"):\n",
    "\t\t\t\t\t\traw_image = raw_image.convert(\"RGB\")\n",
    "\t\t\t\t\traw_image.save(temp_path)\n",
    "\t\t\t\t\traw_image.close()\n",
    "\t\t\t\t\tif out_path in file_list__:\n",
    "\t\t\t\t\t\treturn out_path\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tfile_system__.upload(temp_path, out_path)\n",
    "\t\t\t\t\t\treturn out_path\n",
    "\t\t\t\texcept Exception as ex:\n",
    "\t\t\t\t\tmessage = f\"{x['id']}, {x['subreddit']}, Failure in fetch_image, {ex}, {url}\"\n",
    "\t\t\t\t\tprint(message)\n",
    "\t\t\t\t\treturn \"\"\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn \"\"\n",
    "\texcept Exception as ex:\n",
    "\t\tmessage = f\"{x['id']}, {x['subreddit']}, Failure in fetch_image, {ex}\"\n",
    "\t\tprint(message)\n",
    "\t\treturn \"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Fetch Images\n",
    "\n",
    "file_list = file_system.ls(\"data/image\")\n",
    "\n",
    "for elem in tqdm(file_list, total=len(file_list)):\n",
    "\tif file_system.size(elem) == 0:\n",
    "\t\tprint(f\"I am empty {elem}\")\n",
    "\t\tfile_system.delete(elem)\n",
    "\t\tfile_list.remove(elem)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_list = file_system.ls(\"data/image\")\n",
    "with tqdm(total=len(filtered)) as pbar:\n",
    "\tfor i, row in filtered.iterrows():\n",
    "\t\td = fetch_image(row, file_list, file_system)\n",
    "\t\tfiltered.loc[i, 'path'] = d\n",
    "\t\tpbar.update()\n",
    "\n",
    "display(\"== Filtered Data With Path ==\")\n",
    "display(filtered.shape)\n",
    "display(filtered.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_exists(x: object, file_system__) -> bool:\n",
    "\ttry:\n",
    "\t\tsub_reddit = x['subreddit']\n",
    "\t\trecord_id = x['id']\n",
    "\t\tremote_path = x['path']\n",
    "\t\ttemp_path = f\"temp/image/{sub_reddit}/{record_id}.jpg\"\n",
    "\t\tif os.path.exists(temp_path) and file_system__.exists(remote_path):\n",
    "\t\t\tprint(\"found in both local and remote\")\n",
    "\t\t\treturn True\n",
    "\t\tif os.path.exists(temp_path) and not file_system__.exists(remote_path):\n",
    "\t\t\tfile_system__.upload(temp_path, remote_path)\n",
    "\t\t\tprint(\"Only found in local, downloading to remote\")\n",
    "\t\t\treturn True\n",
    "\t\tif not os.path.exists(temp_path) and file_system__.exists(remote_path):\n",
    "\t\t\tfile_system__.download(remote_path, temp_path)\n",
    "\t\t\tprint(\"Only found in remote, downloading to local\")\n",
    "\t\t\treturn True\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Not found\")\n",
    "\t\t\treturn False\n",
    "\texcept Exception as ex:\n",
    "\t\tprint(ex)\n",
    "\t\treturn False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Apply Exist\n",
    "\n",
    "for i, row in filtered.iterrows():\n",
    "\td = set_exists(row, file_system)\n",
    "\tfiltered.loc[i, 'exists'] = d\n",
    "\n",
    "display(\"== Filtered Data With Exists ==\")\n",
    "display(filtered.shape)\n",
    "display(filtered.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Apply Hash\n",
    "\n",
    "for i, row in filtered.iterrows():\n",
    "\td = functions.set_hash(row)\n",
    "\tfiltered.loc[i, 'hash'] = d\n",
    "display(filtered.shape)\n",
    "display(filtered)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Filter On Exists and path existance\n",
    "\n",
    "ts = datetime.today().timestamp()\n",
    "\n",
    "exists = filtered.loc[(filtered['exists'] == True) & (filtered['path'] != \"\")]\n",
    "exists.dropna(axis=1, how='all')\n",
    "exists.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(exists.shape)\n",
    "display(exists.head())\n",
    "\n",
    "new_records_to_process = exists.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Store New Records\n",
    "\n",
    "\n",
    "out_name = f\"data/temp/caption/primary/{ts}_new_records_to_process.parquet\"\n",
    "new_records_to_process.to_parquet(out_name, engine='pyarrow', filesystem=file_system, schema=schema)\n",
    "\n",
    "new_df = pandas.read_parquet(out_name, engine='pyarrow', filesystem=file_system, schema=schema)\n",
    "\n",
    "display(\"== New Records To Process ==\")\n",
    "display(new_df.shape)\n",
    "display(new_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title End Cache Collection\n",
    "\n",
    "\n",
    "display(f\"Total Number Of New Images - {new_df.shape[0]}\")\n",
    "display(\"0-1 Image Acquisition Complete - Writing new output and Shutting Down\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Import BLIP\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "caption_0 = BlipCaption(\"cuda\")\n",
    "# caption_1 = BlipCaption(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Read Temp Captions\n",
    "\n",
    "ready_to_caption = pandas.read_parquet('data/temp/caption/primary', engine='pyarrow', filesystem=file_system, schema=schema)\n",
    "ready_to_caption.set_index(\"id\", inplace=True, drop=False)\n",
    "ready_to_caption.drop_duplicates(subset=['id'], keep='last', inplace=True)\n",
    "\n",
    "extant_curate = pandas.read_parquet('data/parquet/primary_caption.parquet', engine='pyarrow', filesystem=file_system, schema=schema)\n",
    "extant_curate.set_index(\"id\", inplace=True, drop=False)\n",
    "\n",
    "display(\"=== Extant Data ===\")\n",
    "display(extant_curate.shape)\n",
    "display(extant_curate.head())\n",
    "\n",
    "display(\"=== Read To Caption ===\")\n",
    "display(ready_to_caption.shape)\n",
    "display(ready_to_caption.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Filter on bad data\n",
    "\n",
    "\n",
    "check = ready_to_caption.loc[(ready_to_caption['caption'] == \"\") & (ready_to_caption['caption'].notnull()) & (ready_to_caption['exists'] == True) & (~ready_to_caption['id'].isin(extant_curate.index.values))]\n",
    "\n",
    "ready_to_caption = check.copy()\n",
    "\n",
    "display(\"== Filtered On Existing Caption ==\")\n",
    "display(ready_to_caption.shape)\n",
    "display(ready_to_caption.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Apply Captions\n",
    "\n",
    "with tqdm(total=len(ready_to_caption)) as pbar:\n",
    "\tfor i, row in ready_to_caption.iterrows():\n",
    "\t\tready_to_caption.loc[i, 'caption'] = functions.apply_caption(row, [caption_0, caption_0], file_system)\n",
    "\t\tpbar.update()\n",
    "\n",
    "display(\"== Data With Captions ==\")\n",
    "display(ready_to_caption.shape)\n",
    "display(ready_to_caption.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Drop Missing Captions\n",
    "\n",
    "dropped = ready_to_caption.dropna(axis=1, how='all')\n",
    "dropped.reset_index(drop=True, inplace=True)\n",
    "dropped.set_index(\"id\", inplace=True, drop=False)\n",
    "\n",
    "post_caption_drop = dropped.copy()\n",
    "\n",
    "display(\"== Dropped And Reset Data After Captioning ==\")\n",
    "display(post_caption_drop.shape)\n",
    "display(post_caption_drop.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Ensure Valid\n",
    "\n",
    "foo = extant_curate.index.values\n",
    "bar = post_caption_drop.index.values\n",
    "\n",
    "display(f\"Extant Contains New: {foo in bar} - {bar in foo}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Combine extant_curate and post_caption_drop\n",
    "\n",
    "concat = pandas.concat([extant_curate, post_caption_drop])\n",
    "\n",
    "dropped = concat.dropna(axis=1, how='all')\n",
    "dropped.reset_index(drop=True, inplace=True)\n",
    "dropped.set_index(\"id\", inplace=True, drop=False)\n",
    "\n",
    "display(\"== Dropped And Reset Data ==\")\n",
    "display(dropped.shape)\n",
    "display(dropped)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Create Backup\n",
    "\n",
    "back_up_name = f\"data/parquet/primary_caption_{dt.datetime.timestamp(dt.datetime.now())}.parquet\"\n",
    "\n",
    "display(f\"== Writing Back-Up {back_up_name} ==\")\n",
    "current = pandas.read_parquet('data/parquet/primary_caption.parquet', engine='pyarrow', filesystem=file_system,\n",
    "\t\t\t\t\t\t\t  schema=schema)\n",
    "current.to_parquet(back_up_name, schema=schema, filesystem=file_system)\n",
    "\n",
    "display(current.shape)\n",
    "display(current.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Write Output\n",
    "\n",
    "dropped.reset_index(inplace=True, drop=True)\n",
    "dropped.to_parquet(\"data/parquet/primary_caption.parquet\", schema=schema, filesystem=file_system)\n",
    "\n",
    "display(\"== Output Written ==\")\n",
    "display(dropped.shape)\n",
    "display(dropped)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Plot Summary: By Subreddit\n",
    "\n",
    "primary_caption = pandas.read_parquet(\"data/parquet/primary_caption.parquet\", engine='pyarrow', filesystem=file_system)\n",
    "accepted_primary = primary_caption.where(primary_caption['accept'] == True)\n",
    "accepted_primary.dropna(inplace=True)\n",
    "accepted_primary.reset_index(inplace=True, drop=True)\n",
    "\n",
    "thumbnail_curation = pandas.read_parquet('data/parquet/thumbnail_curation.parquet', engine='pyarrow',\n",
    "\t\t\t\t\t\t\t\t\t\t filesystem=file_system)\n",
    "accepted_thumbnail = thumbnail_curation.where(thumbnail_curation['thumbnail_accept'] == True)\n",
    "accepted_thumbnail.dropna(inplace=True)\n",
    "accepted_thumbnail.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group = accepted_primary[[\"id\", \"subreddit\"]].groupby(\"subreddit\").count().sort_values(by=\"id\", ascending=False)\n",
    "plot = group.plot.bar(figsize=(20, 10), title=\"Subreddits Count Of Accepted Images\", legend=True)\n",
    "display(plot)\n",
    "\n",
    "group = primary_caption[[\"id\", \"subreddit\"]].groupby(\"subreddit\").count().sort_values(by=\"id\", ascending=False)\n",
    "plot = group.plot.bar(figsize=(20, 10), title=\"Subreddits Count Of ALL Images\", legend=True)\n",
    "display(plot)\n",
    "\n",
    "group = accepted_thumbnail[[\"id\", \"subreddit\"]].groupby(\"subreddit\").count().sort_values(by=\"id\", ascending=False)\n",
    "plot = group.plot.bar(figsize=(20, 10), title=\"Curated Count of Accepted Images By Sub\", legend=True)\n",
    "display(plot)\n",
    "\n",
    "group = primary_caption[[\"id\", \"model\"]].groupby(\"model\").count().sort_values(by=\"id\", ascending=False)\n",
    "plot = group.plot.bar(figsize=(20, 10), title=\"All By Model\", legend=True)\n",
    "display(plot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Plot Summary: By Model Type\n",
    "\n",
    "group = accepted_primary[[\"id\", \"model\", \"subreddit\"]].groupby([\"model\"]).count().sort_values(by=\"id\", ascending=False)\n",
    "plot = group.plot.bar(figsize=(20, 10), title=\"Models with most images\", legend=True)\n",
    "display(plot)\n",
    "\n",
    "group = accepted_thumbnail[[\"id\", \"model\", \"subreddit\"]].groupby([\"model\"]).count().sort_values(by=\"id\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tascending=False)\n",
    "plot_1 = group.plot.bar(figsize=(20, 10), title=\"Models with most images\", legend=True)\n",
    "display(plot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #@title Disconnect\n",
    "#\n",
    "# from google.colab import runtime\n",
    "# runtime.unassign()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
